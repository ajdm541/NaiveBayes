{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Training Data\n",
    "path = \"trg.csv\"\n",
    "\n",
    "#Not Pandas\n",
    "with open(path) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    data = list(readCSV)\n",
    "    headers = data[0]\n",
    "    data = data[1:]\n",
    "    data = np.array(data)\n",
    "    \n",
    "path = \"tst.csv\"\n",
    "with open(path) as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    dataTesting = list(readCSV)\n",
    "    headersTesting = dataTesting[0]\n",
    "    dataTesting = dataTesting[1:]\n",
    "    dataTesting = np.array(dataTesting)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing - NO PANDAS\n",
    "#Format - Uppercase letters\n",
    "for row in data:\n",
    "    row[2]=row[2].upper()\n",
    "    \n",
    "#Remove negavite numbers and special characters (')\n",
    "for row in data:\n",
    "    row[2]=re.sub(r\"\\s(-*\\d*)*\\s\",' ',row[2])\n",
    "    row[2]=re.sub(r\"[']\",' ',row[2])\n",
    "\n",
    "#Split abstract test by words\n",
    "index = 0\n",
    "abstracts = {}\n",
    "for row in data:\n",
    "    abstracts[index] = np.array(row[2].split(' '))\n",
    "    index += 1\n",
    "    \n",
    "#Remove common words and sort the array\n",
    "#Function to remove common words\n",
    "def removeCommon(words):\n",
    "    #List of common words\n",
    "    remove = ['THE','OF','A','AN','FOR','THAT','WITH','BY','AND',\n",
    "          'OR','IS','ARE','BY','WAS','WERE','IT','ITS', 'TO',\n",
    "             'WHICH', 'IN', 'HAVE', 'HAS', 'NO','NOT','AS', 'ALSO']\n",
    "    \n",
    "    #Loop to remove common words & numeric values in the abstract\n",
    "    control=0\n",
    "    while control<len(words):\n",
    "        if words[control] in remove:\n",
    "            words[control]=\"-DELETE-\"\n",
    "        control += 1\n",
    "    words = np.delete(words,np.argwhere(words=='-DELETE-')[:,0])\n",
    "    \n",
    "    #Return the sorted array    \n",
    "    words = np.sort(words)\n",
    "    return words\n",
    "\n",
    "#Words Collector\n",
    "# Identify different words in the abstracts \n",
    "# and count number of abstracts in which their appears (frequency in data instances - NOT Individual)\n",
    "def wordCollector(column):\n",
    "    globalWords = {}\n",
    "    for text in column:\n",
    "        rowWords = {}\n",
    "        for new in column[text]:\n",
    "            if not new in rowWords.keys():\n",
    "                rowWords[new]=1;\n",
    "        for word in rowWords.keys():\n",
    "            if word in globalWords.keys():\n",
    "                globalWords[word] += 1\n",
    "            else:\n",
    "                globalWords[word] = 1\n",
    "    return globalWords\n",
    "\n",
    "#Remove Outliers\n",
    "def removeUnique(allWords, limit=1):\n",
    "    words = list(allWords)\n",
    "    for word in words:\n",
    "        if allWords[word]<=limit:\n",
    "            allWords.pop(word)\n",
    "    return allWords       \n",
    "\n",
    "#Calculate probability of each class in the given class column of the dataset\n",
    "def classProbability(classes):\n",
    "    classProbs = {}\n",
    "    uniqueClass = np.unique(classes)\n",
    "    for c in uniqueClass:\n",
    "        classProbs[c] = np.count_nonzero(classes==c)/classes.size\n",
    "    return classProbs\n",
    "\n",
    "#Function to group all preprocessing steps\n",
    "def preprocessing(data, unlabel=False):\n",
    "    if unlabel==True:\n",
    "        index=1\n",
    "    else:\n",
    "        index=2\n",
    "    #Format - Uppercase letters\n",
    "    for row in data:\n",
    "        row[index]=row[index].upper()\n",
    "        \n",
    "    #Remove negavite numbers and special characters (')\n",
    "    for row in data:\n",
    "        row[index]=re.sub(r\"\\s(-*\\d*)*\\s\",' ',row[index])\n",
    "        row[index]=re.sub(r\"[']\",' ',row[index])\n",
    "        \n",
    "    #Split abstract string\n",
    "    num = 0\n",
    "    abstracts = {}\n",
    "    for row in data:\n",
    "        abstracts[num] = np.array(row[index].split(' '))\n",
    "        num += 1\n",
    "        \n",
    "    #RemoveCommon words\n",
    "    for element in abstracts:\n",
    "        abstracts[element] = removeCommon(abstracts[element])\n",
    "        \n",
    "    return abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Model - NO PANDAS\n",
    "\n",
    "#General Functions\n",
    "\n",
    "#Calculate count of word of instances for a given class\n",
    "#target = class to count words\n",
    "#data = dataset - abstracts\n",
    "#instances = index of instances of the target class\n",
    "def classWords(target, data, instances):\n",
    "    wordFrequency = {}\n",
    "    cWords = {}\n",
    "    cWords['total']=0\n",
    "    for row in instances:\n",
    "        wordsRow = np.unique(data[row])\n",
    "        for word in data[row]:\n",
    "            if not word in cWords.keys():\n",
    "                cWords[word] = 1\n",
    "                wordFrequency[word]=1\n",
    "                wordsRow = np.delete(wordsRow,np.where(wordsRow==word)[0])\n",
    "            else:\n",
    "                cWords[word] += 1\n",
    "                if word in wordsRow:\n",
    "                    wordFrequency[word]+=1\n",
    "                    wordsRow = np.delete(wordsRow,np.where(wordsRow==word)[0])\n",
    "            cWords['total']+=1\n",
    "    \n",
    "    wordFrequency = removeUnique(wordFrequency, 1)\n",
    "    wordFrequency['total']=cWords['total']\n",
    "    return cWords, wordFrequency\n",
    "\n",
    "#Calculate the prior probabilities of an instances for all classe\n",
    "#words = Vocabulary of the dataset\n",
    "#instance = instances | row evaluated\n",
    "#targets = predictable classes\n",
    "#targetwords = count of word of all instances of the same class\n",
    "def conditionalProb(words, instances, targets, targetWords):\n",
    "    classPriorProbs = {}\n",
    "    \n",
    "    #Data Vocabulary (Count of unique words)\n",
    "    wordCount = len(words)\n",
    "    abstractWords = np.unique(instances)\n",
    "    \n",
    "    # Evaluation per class\n",
    "    for target in targets:        \n",
    "        classPriorProbs[target] = {}\n",
    "        totalClassWords = targetWords[target]['total']\n",
    "        temp = targetWords[target].pop('total')\n",
    "\n",
    "        #Evaluation of each word in the instances        \n",
    "        for word in abstractWords:\n",
    "            if not word in targetWords[target].keys():\n",
    "                classPriorProbs[target][word] = 1 / (totalClassWords + wordCount)\n",
    "            else:\n",
    "                classPriorProbs[target][word] = \\\n",
    "                (targetWords[target][word] + 1 ) / (totalClassWords + wordCount)\n",
    "        targetWords[target]['total']=temp\n",
    "    return classPriorProbs\n",
    "\n",
    "#Classifier - Calculate class probability of and instance\n",
    "#instance = instance evaluated\n",
    "#condProb = conditional probabilities - classPriorProbs - prob of word for each class\n",
    "#priorProb = Prior probabilities for each class\n",
    "def classProb(instance, condProbs, priorProb):\n",
    "    postProb = {}\n",
    "    words = {}\n",
    "    \n",
    "    # Count unique word of an instance\n",
    "    rowWords = np.unique(instance)\n",
    "    for word in rowWords:        \n",
    "        words[word] = np.count_nonzero(instance==word)\n",
    "        \n",
    "    # Count unique words in a class\n",
    "    for x in priorProb.keys():\n",
    "        postProb[x] = math.log(priorProb[x])\n",
    "        \n",
    "        # Calculate posterior probability of instance\n",
    "        for word in words:\n",
    "            postProb[x] += words[word]*math.log(condProbs[x][word])   \n",
    "    return postProb\n",
    "\n",
    "#Performances - Asses acurracy of model\n",
    "#predictions - Array with predicted classes\n",
    "#real - array of data with target classes from the original dataset (id, class, abstract)\n",
    "def modelPerformance(predictions, real):\n",
    "    valid=0;\n",
    "    total = len(predictions)\n",
    "    for prediction in range(len(predictions)):\n",
    "        if predictions[prediction]==real[prediction][1]:\n",
    "            valid += 1\n",
    "    performance=valid/total\n",
    "    return performance\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Fold lenght:3600\n",
      "Total number of words:27651\n",
      "Total number of words after remove unique :12358\n",
      "Fold: 1\n",
      "Fold lenght:3600\n",
      "Total number of words:27678\n",
      "Total number of words after remove unique :12279\n",
      "Fold: 2\n",
      "Fold lenght:3600\n",
      "Total number of words:27584\n",
      "Total number of words after remove unique :12183\n",
      "Fold: 3\n",
      "Fold lenght:3600\n",
      "Total number of words:27725\n",
      "Total number of words after remove unique :12297\n",
      "Fold: 4\n",
      "Fold lenght:3600\n",
      "Total number of words:27762\n",
      "Total number of words after remove unique :12237\n",
      "Fold: 5\n",
      "Fold lenght:3600\n",
      "Total number of words:27921\n",
      "Total number of words after remove unique :12317\n",
      "Fold: 6\n",
      "Fold lenght:3600\n",
      "Total number of words:27946\n",
      "Total number of words after remove unique :12314\n",
      "Fold: 7\n",
      "Fold lenght:3600\n",
      "Total number of words:27873\n",
      "Total number of words after remove unique :12266\n",
      "Fold: 8\n",
      "Fold lenght:3600\n",
      "Total number of words:27811\n",
      "Total number of words after remove unique :12328\n",
      "Fold: 9\n",
      "Fold lenght:3600\n",
      "Total number of words:27781\n",
      "Total number of words after remove unique :12230\n",
      "0.907\n"
     ]
    }
   ],
   "source": [
    "#Cross-Validation Test\n",
    "folds = 10\n",
    "samplesData = {}\n",
    "cycle = 0\n",
    "performance = np.zeros(folds)\n",
    "\n",
    "#Split data \n",
    "#Creating Folds\n",
    "foldSize= int(len(data)/folds)\n",
    "initialSample = 0\n",
    "finalSample = foldSize\n",
    "for fold in range(folds):\n",
    "    if fold == folds:\n",
    "        samplesData[fold] = data[initialSample:,0:3]\n",
    "    else:\n",
    "        samplesData[fold] = data[initialSample:finalSample,0:3]\n",
    "        initialSample = finalSample\n",
    "        finalSample += foldSize\n",
    "        \n",
    "#Evaluate model performances with cross-validation\n",
    "for sample in range(folds):\n",
    "    trainingData = []   # id, class\n",
    "    print(\"Fold: \" + str(sample))\n",
    "    testData = samplesData[sample]     #id, class, abstracts\n",
    "    for trainingSample in samplesData:        \n",
    "        if sample!=trainingSample:\n",
    "            for x in range(len(samplesData[trainingSample])):\n",
    "                trainingData.append(samplesData[trainingSample][x])\n",
    "\n",
    "    trainingData = np.array(trainingData)\n",
    "    \n",
    "    #Determine Instances\n",
    "    numInstances = len(trainingData)\n",
    "    print(\"Fold lenght:\" + str(numInstances))\n",
    "    \n",
    "    #Preprocessing training Data\n",
    "    training = preprocessing(trainingData)\n",
    "    test = preprocessing(testData)\n",
    "    \n",
    "    #Select training\n",
    "    #training = abstracts\n",
    "    \n",
    "    #Determine classes\n",
    "    classes = np.unique(trainingData[:,1])\n",
    "    \n",
    "    #Determine class probabilities\n",
    "    classProbs = classProbability(trainingData[:,1])\n",
    "    \n",
    "    #Calculate words frequency\n",
    "    allWords = wordCollector(training)\n",
    "    print(\"Total number of words:\" + str(len(allWords)))\n",
    "    \n",
    "    allWords = removeUnique(allWords, 1)\n",
    "    print(\"Total number of words after remove unique :\" + str(len(allWords)))\n",
    "    \n",
    "    #Create and array with all words\n",
    "    words = list(allWords)\n",
    "    words.sort()\n",
    "    \n",
    "    #Get words count of instances of each class value\n",
    "    classesWords = {}\n",
    "    frequentWords = {}\n",
    "    for c in classes:\n",
    "        classesWords[c], frequentWords[c]= classWords(c, training, np.where(trainingData==c)[0])\n",
    "   \n",
    "    testSamples=len(test)\n",
    "    predictions=[]\n",
    "    for elem in range(len(test)):\n",
    "        \n",
    "        #Get conditional probabilities of rows\n",
    "        condProbs = conditionalProb(words,test[elem],classes,classesWords)\n",
    "        \n",
    "        #Classifier - Selecting a class\n",
    "        classifier = classProb(test[elem], condProbs, classProbs)\n",
    "        \n",
    "        #Select final classification\n",
    "        predictions.append(max(classifier.keys(), key=(lambda k: classifier[k])))\n",
    "    \n",
    "    #Measure fold performance\n",
    "    performance[sample]= modelPerformance(predictions,testData)\n",
    "    averagePerformance = sum(performance)/len(performance)\n",
    "print(averagePerformance)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Solution and Export to CSV\n",
    "with open('trainingResults.csv', mode='w', newline='') as csv_file:\n",
    "    fieldnames = headers\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "\n",
    "    writer.writerow([headers[0],headers[1],\"Real\"])\n",
    "    for index in range(len(predictions)):\n",
    "        writer.writerow([index+1, predictions[index],testData[index][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'A', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'A', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'A', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'A', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'A', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'A', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'A', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'A', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'A', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'V', 'B', 'E', 'B', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'V', 'E', 'E', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'A', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'V', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'E', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'B', 'E', 'B', 'E', 'B', 'B', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'B', 'B', 'B', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'B', 'E', 'E', 'B', 'B', 'E', 'E', 'B', 'E', 'E', 'E', 'E', 'B', 'E', 'A', 'B', 'E', 'E', 'E', 'E', 'E', 'E', 'E', 'A', 'B']\n"
     ]
    }
   ],
   "source": [
    "#Classify test data tst.csv\n",
    "\n",
    "test = preprocessing(dataTesting, True)\n",
    "\n",
    "testSamples=len(test)\n",
    "predictions=[]\n",
    "for elem in range(len(test)):\n",
    "        \n",
    "    #Get conditional probabilities of rows\n",
    "    condProbs = conditionalProb(words,test[elem][1],classes,classesWords)\n",
    "        \n",
    "    #Classifier - Selecting a class\n",
    "    classifier = classProb(test[elem][1], condProbs, classProbs)\n",
    "        \n",
    "    #Select final classification\n",
    "    predictions.append(max(classifier.keys(), key=(lambda k: classifier[k])))\n",
    "    \n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Solution and Export to CSV\n",
    "with open('amon897.csv', mode='w', newline='') as csv_file:\n",
    "    fieldnames = headers\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "\n",
    "    writer.writerow([headers[0],headers[1]])\n",
    "    for index in range(len(predictions)):\n",
    "        writer.writerow([index+1, predictions[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

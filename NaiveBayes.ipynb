{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports and libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Training Data\n",
    "path = \"trg.csv\"\n",
    "#Pandas\n",
    "RawData = pd.read_csv(\"trg.csv\",index_col=0)\n",
    "RawData.head(n=10)\n",
    "\n",
    "#Not Pandas\n",
    "with open(\"trg.csv\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=',')\n",
    "    data = list(readCSV)\n",
    "    headers = data[0]\n",
    "    data = data[1:]\n",
    "    data = np.array(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:34095\n",
      "Total number of words after remove unique :14348\n",
      "{'B': 0.4005, 'A': 0.032, 'E': 0.536, 'V': 0.0315}\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing - PANDAS\n",
    "#Format - Uppercase letters\n",
    "RawData['abstract'] = RawData['abstract'].str.upper()\n",
    "\n",
    "\n",
    "#Remove negavite numbers and special characters (')\n",
    "RawData['abstract'] = RawData['abstract'].str.replace(r\"\\s(-*\\d*)*(-*\\d*)*$\\s|(-$)'\",'',regex=True)\n",
    "RawData['abstract'] = RawData['abstract'].str.replace(r\"\\s(-*\\d*)*\\s\",'',regex=True)\n",
    "RawData['abstract'] = RawData['abstract'].str.replace(r\"\\s(-?\\d+)\\s\",'',regex=True)\n",
    "RawData['abstract'] = RawData['abstract'].str.replace(\"'\",'',regex=False)\n",
    "\n",
    "#Split abstract string\n",
    "RawData['abstract'] = RawData['abstract'].str.split(' ')\n",
    "\n",
    "    \n",
    "#Remove common words and sort the array\n",
    "#Function to remove common words\n",
    "def removeCommon(array):\n",
    "    remove = ['THE','OF','A','AN','FOR','THAT','WITH','BY','AND',\n",
    "          'OR','IS','ARE','BY','WAS','WERE','IT','ITS', 'TO',\n",
    "             'WHICH', 'IN', 'HAVE', 'HAS', 'NO','NOT','AS', 'ALSO']\n",
    "    #Loop to remove common words & numeric values in the abstract\n",
    "    control=0\n",
    "    while control<len(array):\n",
    "        if array[control] in remove:\n",
    "            array.pop(control)\n",
    "        else:\n",
    "            if array[control].isnumeric():\n",
    "                array.pop(control)\n",
    "            else:\n",
    "                control += 1\n",
    "    #Return the sorted array    \n",
    "    array = array.sort()\n",
    "    return array\n",
    "\n",
    "#for word in remove:\n",
    "#    control=1\n",
    "#    while control<=len(array):\n",
    "#        if array[control] in remove:\n",
    "#            array.pop(word)\n",
    "#    while len(array)>0 and array.pop()=word or control>len(array):\n",
    "#        array.pop(word)\n",
    "#        control++\n",
    "        \n",
    "#Apply function to the column data\n",
    "RawData['abstract'].map(lambda x: removeCommon(x))\n",
    "\n",
    "#Words Collector\n",
    "# Identify different words in the abstracts \n",
    "# and count number of abstracts in which their appears \n",
    "def wordCollector(column):\n",
    "    globalWords = {}\n",
    "    for text in column:\n",
    "        rowWords = {}\n",
    "        for word in text:\n",
    "            if not word in rowWords.keys():\n",
    "                rowWords[word]=1;\n",
    "        for new in rowWords.keys():\n",
    "            if new in globalWords.keys():\n",
    "                globalWords[new] += 1\n",
    "            else:\n",
    "                globalWords[new] = 1\n",
    "    return globalWords\n",
    "\n",
    "allWords = wordCollector(RawData['abstract'])\n",
    "print(\"Total number of words:\" + str(len(allWords)))\n",
    "\n",
    "#Remove Outliers\n",
    "def removeUnique(allWords, limit=1):\n",
    "    words = list(allWords)\n",
    "    for word in words:\n",
    "        if allWords[word]<=limit:\n",
    "            allWords.pop(word)\n",
    "    return allWords       \n",
    "\n",
    "allWords = removeUnique(allWords, 1)\n",
    "print(\"Total number of words after remove unique :\" + str(len(allWords)))\n",
    "\n",
    "#Create and array with all words\n",
    "words = list(allWords)\n",
    "words.sort()\n",
    "\n",
    "#Create classes array\n",
    "classes = RawData['class'].unique()\n",
    "\n",
    "#Calculate probability of each class in the given class column of the dataset\n",
    "def classProbability(data):\n",
    "    classProbs = {}\n",
    "    classes = data.unique()\n",
    "    for c in classes:\n",
    "        classProbs[c] = data.where(data==c).count()/data.count()\n",
    "    return classProbs\n",
    "classprobs = classProbability(RawData['class'])\n",
    "print(classprobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words:29810\n",
      "Total number of words after remove unique :13191\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing - NO PANDAS\n",
    "#Format - Uppercase letters\n",
    "for row in data:\n",
    "    row[2]=row[2].upper()\n",
    "#    print(data[0:2])\n",
    "    \n",
    "#Remove negavite numbers and special characters (')\n",
    "for row in data:\n",
    "    row[2]=re.sub(r\"\\s(-*\\d*)*\\s\",' ',row[2])\n",
    "    row[2]=re.sub(r\"[']\",' ',row[2])\n",
    "\n",
    "#Split abstract string\n",
    "index = 0\n",
    "abstracts = {}\n",
    "for row in data:\n",
    "    abstracts[index] = np.array(row[2].split(' '))\n",
    "    index += 1\n",
    "    \n",
    "#Remove common words and sort the array\n",
    "#Function to remove common words\n",
    "def removeCommon(words):\n",
    "#List of common words\n",
    "    remove = ['THE','OF','A','AN','FOR','THAT','WITH','BY','AND',\n",
    "          'OR','IS','ARE','BY','WAS','WERE','IT','ITS', 'TO',\n",
    "             'WHICH', 'IN', 'HAVE', 'HAS', 'NO','NOT','AS', 'ALSO']\n",
    "#Loop to remove common words & numeric values in the abstract\n",
    "    control=0\n",
    "    while control<len(words):\n",
    "        if words[control] in remove:\n",
    "            words[control]=\"-DELETE-\"\n",
    "        control += 1\n",
    "    words = np.delete(words,np.argwhere(words=='-DELETE-')[:,0])\n",
    "#Return the sorted array    \n",
    "    words = np.sort(words)\n",
    "    return words\n",
    "       \n",
    "#Apply function to the column data\n",
    "for element in abstracts:\n",
    "    abstracts[element] = removeCommon(abstracts[element])\n",
    "\n",
    "#Words Collector\n",
    "# Identify different words in the abstracts \n",
    "# and count number of abstracts in which their appears (frequency in data instances - NOT Individual)\n",
    "def wordCollector(column):\n",
    "    globalWords = {}\n",
    "    for text in column:\n",
    "        rowWords = {}\n",
    "        for new in column[text]:\n",
    "            if not new in rowWords.keys():\n",
    "                rowWords[new]=1;\n",
    "        for word in rowWords.keys():\n",
    "            if word in globalWords.keys():\n",
    "                globalWords[word] += 1\n",
    "            else:\n",
    "                globalWords[word] = 1\n",
    "    return globalWords\n",
    "\n",
    "allWords = wordCollector(abstracts)\n",
    "print(\"Total number of words:\" + str(len(allWords)))\n",
    "\n",
    "#Remove Outliers\n",
    "def removeUnique(allWords, limit=1):\n",
    "    words = list(allWords)\n",
    "    for word in words:\n",
    "        if allWords[word]<=limit:\n",
    "            allWords.pop(word)\n",
    "    return allWords       \n",
    "\n",
    "allWords = removeUnique(allWords, 1)\n",
    "print(\"Total number of words after remove unique :\" + str(len(allWords)))\n",
    "\n",
    "#Create and array with all words\n",
    "words = list(allWords)\n",
    "words.sort()\n",
    "\n",
    "#Create classes array\n",
    "classes = np.unique(data[:,1])\n",
    "\n",
    "#Calculate probability of each class in the given class column of the dataset\n",
    "def classProbability(classes):\n",
    "    classProbs = {}\n",
    "    uniqueClass = np.unique(classes)\n",
    "    for c in uniqueClass:\n",
    "        classProbs[c] = np.count_nonzero(classes==c)/classes.size\n",
    "    return classProbs\n",
    "#classProbs = classProbability(data[:,1])\n",
    "#print(classProbs)\n",
    "\n",
    "def preprocessing(data):\n",
    "    #Format - Uppercase letters\n",
    "    for row in data:\n",
    "        row[2]=row[2].upper()\n",
    "        \n",
    "    #Remove negavite numbers and special characters (')\n",
    "    for row in data:\n",
    "        row[2]=re.sub(r\"\\s(-*\\d*)*\\s\",' ',row[2])\n",
    "        row[2]=re.sub(r\"[']\",' ',row[2])\n",
    "        \n",
    "    #Split abstract string\n",
    "    index = 0\n",
    "    abstracts = {}\n",
    "    for row in data:\n",
    "        abstracts[index] = np.array(row[2].split(' '))\n",
    "        index += 1\n",
    "        \n",
    "    #RemoveCommon words\n",
    "    for element in abstracts:\n",
    "        abstracts[element] = removeCommon(abstracts[element])\n",
    "        \n",
    "    return abstracts\n",
    "#   \n",
    "#    #Count frequency of words in Data\n",
    "#    allWords = wordCollector(abstracts)\n",
    "#    #words = list(allWords)\n",
    "#    #words.sort()\n",
    "#    print(\"Total number of words:\" + str(len(allWords)))\n",
    "#    \n",
    "#    allWords = removeUnique(allWords, 1)\n",
    "#    print(\"Total number of words after remove unique :\" + str(len(allWords)))\n",
    "#    \n",
    "#    #Create and array with all words\n",
    "#    words = list(allWords)\n",
    "#    words = words.sort()\n",
    "#    \n",
    "#    #Create classes array\n",
    "#    classes = np.unique(data[:,1])\n",
    "#    \n",
    "#    #Calculate classes probabilities\n",
    "#    classProbs = classProbability(data[:,1])\n",
    "#    #print(classProbs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training = np.asarray(list(abstracts.items()))\n",
    "#print(training[:2,1])\n",
    "#training = np.asarray(list(abstracts.items())[:10])\n",
    "#training = np.asarray(abstracts)\n",
    "\n",
    "#for row in data:\n",
    "#    row[2]=row[2].upper()\n",
    "\n",
    "#print(data[0:2])\n",
    "\n",
    "#classData = RawData.loc[RawData['class'] == 'E']\n",
    "#classData.head(n=5)\n",
    "#print(classData.size)\n",
    "\n",
    "#allWords = wordCollector(RawData['abstract'])\n",
    "#words = list(allWords)\n",
    "#words.sort()\n",
    "#print(words)\n",
    "#print(allWords)\n",
    "#RawData['abstract'].map(lambda x: removeCommon(x))\n",
    "#RawData['abstract'].head(n=10)\n",
    "#RawData.head(n=5)\n",
    "#new = removeCommon(RawData['abstract'][1])\n",
    "#print(new)\n",
    "\n",
    "#removeCommon(RawData['abstract'][1])\n",
    "#print(RawData['abstract'][1])\n",
    "\n",
    "    #classPriorProbs = np.zeros(len(targets))\n",
    "  #wordCount = len(dict.fromkeys(instances))\n",
    "\n",
    "    #for c in classPriorProbs:\n",
    "    #    c = np.zeros(len(words))\n",
    "\n",
    "    \n",
    "            #targetWords[target].pop('total')\n",
    "        #countClassWords = len(targetWords[target].keys())\n",
    "        \n",
    "        #        for prob in classPriorProbs[x]:   \n",
    "#            postProb[x] =+ instance.count()*math.log(classPriorProbs[x][prob])\n",
    "#        print(postProb)  \n",
    "#        print(classCount, classTotal)\n",
    "\n",
    "        #countClassWords = len(targetWords[target].keys())\n",
    "    \n",
    "    #training = np.asarray(list(abstracts.items())[:10])\n",
    "#training = np.asarray(abstracts)\n",
    "\n",
    "#print(training.loc[training['class']=='A']['abstract'])\n",
    "#print(classWords)\n",
    "#print(classWords.keys())\n",
    "#print(training['abstract'][1])\n",
    "\n",
    "\n",
    "\n",
    "#classData = RawData.loc[RawData['class'] == 'E']\n",
    "#training.head(n=10)\n",
    "#classData.head(n=5)\n",
    "\n",
    "#print(priorProbs)\n",
    "#print(training.loc[training['class']=='A']['abstract'])\n",
    "#print(classWords)\n",
    "#print(classWords.keys())\n",
    "#print(training['abstract'][1])\n",
    "\n",
    "\n",
    "#    testFold = DataTraining[cycle]\n",
    "#    trainingFolds = DataTraining.pop(cycle)\n",
    "#    performance[cycle] = runtest(trainingFolds, testFold)\n",
    "#    cycle++ \n",
    "#cycle = 0\n",
    "#averagePerformance = np.sum(performance)/folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': -12.97336932169116, 'A': -16.184723521634215, 'E': -15.429354947601855, 'V': -19.025625059801197}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'B'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Naive Bayes Model - PANDAS\n",
    "\n",
    "#General Functions\n",
    "\n",
    "#Calculate count of word for a given class\n",
    "def classWords(target, instances):\n",
    "    classWords = {}\n",
    "    classWords['total']=0\n",
    "    for text in instances:\n",
    "        for word in text:\n",
    "            if not word in classWords.keys():\n",
    "                classWords[word] = 1\n",
    "            else:\n",
    "                classWords[word] += 1\n",
    "            classWords['total']+=1\n",
    "    return classWords\n",
    "\n",
    "#Caculate the prior probabilities of an instances for all classes\n",
    "def rowProb(words, instances, targets, targetWords):\n",
    "    classPriorProbs = {}\n",
    "    #Data Vocabulary (Count of unique words)\n",
    "    wordCount = len(words)\n",
    "    abstractWords = dict.fromkeys(instances)\n",
    "# Evaluation per class\n",
    "    for target in targets:        \n",
    "        classPriorProbs[target] = {}\n",
    "        totalClassWords = targetWords[target]['total']\n",
    "#Evaluation of each word in the instances        \n",
    "        for word in abstractWords:\n",
    "            if not word in targetWords[target].keys():\n",
    "                classPriorProbs[target][word] = 1 / (totalClassWords + wordCount)\n",
    "            else:\n",
    "                classPriorProbs[target][word] = \\\n",
    "                (targetWords[target][word] + 1 ) / (totalClassWords + wordCount)\n",
    "    return classPriorProbs\n",
    "\n",
    "def postProbBasic():\n",
    "    return False\n",
    "\n",
    "#Calculate \n",
    "def postProb(instance, classPriorProbs,classColumn):\n",
    "    postProb = {}\n",
    "    words = {}\n",
    "# Count unique word of an instance\n",
    "    for word in dict.fromkeys(instance):\n",
    "        words[word] = instance.count(word)\n",
    "# Count unique words in a class\n",
    "    for x in classColumn.unique():\n",
    "        classCount = classColumn.where(classColumn==x).count()\n",
    "        classTotal = classColumn.count()\n",
    "        postProb[x] =+ math.log(classCount/classTotal)\n",
    "# Calculate posterior probability of instance\n",
    "        for word in words:\n",
    "            postProb[x] += words[word]*math.log(classPriorProbs[x][word])\n",
    " \n",
    "    return postProb\n",
    "\n",
    "def classCode(classes,postProb):\n",
    "    prediction = {}\n",
    "    pre = min(postProb, key=postProb.get)\n",
    "    return prediction\n",
    "\n",
    "\n",
    "#Example\n",
    "#Select training\n",
    "training = RawData.loc[0:10]\n",
    "#Determine classes\n",
    "classes = RawData['class'].unique()\n",
    "#Get words count of instances of each class value\n",
    "classesWords = {}\n",
    "for x in classes:\n",
    "    classesWords[x] = classWords(x,training.loc[training['class']==x]['abstract'])\n",
    "\n",
    "priorProbs = rowProb(words,training['abstract'][1],classes,classesWords)\n",
    "\n",
    "classifier = postProb(training['abstract'][1], priorProbs,training['class'])\n",
    "print(classifier)\n",
    "max(classifier.keys(), key=(lambda k: classifier[k]))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Model - NO PANDAS\n",
    "\n",
    "#General Functions\n",
    "\n",
    "#Calculate count of word of instances for a given class\n",
    "#target = class to count words\n",
    "#data = dataset\n",
    "#instances = index of instances of the target class\n",
    "def classWords(target, data, instances):\n",
    "    cWords = {}\n",
    "    cWords['total']=0\n",
    "    for row in instances:\n",
    "        for word in data[row]:\n",
    "            if not word in cWords.keys():\n",
    "                cWords[word] = 1\n",
    "            else:\n",
    "                cWords[word] += 1\n",
    "            cWords['total']+=1\n",
    "    return cWords\n",
    "\n",
    "#Calculate the prior probabilities of an instances for all classe\n",
    "#words = Vocabulary of the dataset\n",
    "#instance = instances | row evaluated\n",
    "#targets = predictable classes\n",
    "#targetwords = count of word of all instances of the same class\n",
    "def conditionalProb(words, instances, targets, targetWords):\n",
    "    classPriorProbs = {}\n",
    "#Data Vocabulary (Count of unique words)\n",
    "    wordCount = len(words)\n",
    "    abstractWords = np.unique(instances)\n",
    "# Evaluation per class\n",
    "    for target in targets:        \n",
    "        classPriorProbs[target] = {}\n",
    "        totalClassWords = targetWords[target]['total']\n",
    "        temp = targetWords[target].pop('total')\n",
    "\n",
    "#Evaluation of each word in the instances        \n",
    "        for word in abstractWords:\n",
    "            if not word in targetWords[target].keys():\n",
    "                classPriorProbs[target][word] = 1 / (totalClassWords + wordCount)\n",
    "            else:\n",
    "                classPriorProbs[target][word] = \\\n",
    "                (targetWords[target][word] + 1 ) / (totalClassWords + wordCount)\n",
    "        targetWords[target]['total']=temp\n",
    "    return classPriorProbs\n",
    "\n",
    "def postProbBasic():\n",
    "    return False\n",
    "\n",
    "#Classifier - Calculate class probability of and instance\n",
    "#instance = instance evaluated\n",
    "#condProb = conditional probabilities - classPriorProbs - prob of word for each class\n",
    "#priorProb = Prior probabilities for each class\n",
    "def classProb(instance, condProbs, priorProb):\n",
    "    postProb = {}\n",
    "    words = {}\n",
    "# Count unique word of an instance\n",
    "    for word in np.unique(instance[1]):        \n",
    "        words[word] = np.count_nonzero(instance[1]==word)\n",
    "# Count unique words in a class\n",
    "    for x in priorProb.keys():\n",
    "        postProb[x] = math.log(priorProb[x])\n",
    "# Calculate posterior probability of instance\n",
    "        for word in words:\n",
    "            postProb[x] += words[word]*math.log(condProbs[x][word])   \n",
    "    return postProb\n",
    "\n",
    "def classCode(classes,postProb):\n",
    "    prediction = {}\n",
    "    pre = min(postProb, key=postProb.get)\n",
    "    return prediction\n",
    "\n",
    "#Performances - Asses acurracy of model\n",
    "def modelPerformance(predictions, real):\n",
    "    valid=0;\n",
    "    total = len(predictions)\n",
    "    for prediction in range(len(predictions)):\n",
    "        if predictions[prediction]==real[prediction][1]:\n",
    "            valid += 1\n",
    "    performance=valid/total\n",
    "    return performance\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-c080168f015a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mclassesWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mclassesWords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnumInstances\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-130-cd2a73cd2bb4>\u001b[0m in \u001b[0;36mclassWords\u001b[1;34m(target, data, instances)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m                 \u001b[0mcWords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "#Example\n",
    "#Select training\n",
    "training = list(abstracts.items())[:1000]\n",
    "\n",
    "#Determine classes\n",
    "classes = np.unique(data[:,1])\n",
    "\n",
    "#Determine Instances\n",
    "numInstances = len(training)\n",
    "\n",
    "#Get words count of instances of each class value\n",
    "classesWords = {}\n",
    "for c in classes:\n",
    "    classesWords[c] = classWords(c, training, np.where(data[:numInstances]==c)[0])\n",
    "\n",
    "prediction =[]\n",
    "for elem in training:\n",
    "    \n",
    "#Get conditional probabilities of rows\n",
    "    condProbs = conditionalProb(words,training[elem[0]],classes,classesWords)\n",
    "\n",
    "#Classifier - Selecting a class\n",
    "    classifier = classProb(training[elem[0]], condProbs, classProbs)\n",
    "    #print(classifier)\n",
    "    prediction.append(max(classifier.keys(), key=(lambda k: classifier[k])))\n",
    "#print(np.array(prediction))\n",
    "#print(data[:numInstances,1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['401' 'B'\n",
      "  'HELICOBACTER PYLORI ONE OF THE MOST COMMON BACTERIAL PATHOGENS OF HUMANS COLONIZES THE GASTRIC MUCOSA WHERE IT APPEARS TO PERSIST THROUGHOUT THE HOST S LIFE UNLESS THE PATIENT IS TREATED COLONIZATION INDUCES CHRONIC GASTRIC INFLAMMATION WHICH CAN PROGRESS TO A VARIETY OF DISEASES RANGING IN SEVERITY FROM SUPERFICIAL GASTRITIS AND PEPTIC ULCER TO GASTRIC CANCER AND MUCOSAL-ASSOCIATED LYMPHOMA STRAIN-SPECIFIC GENETIC DIVERSITY HAS BEEN PROPOSED TO BE INVOLVED IN THE ORGANISM S ABILITY TO CAUSE DIFFERENT DISEASES OR EVEN BE BENEFICIAL TO THE INFECTED HOST AND TO PARTICIPATE IN THE LIFELONG CHRONICITY OF INFECTION HERE WE COMPARE THE COMPLETE GENOMIC SEQUENCES OF TWO UNRELATED H PYLORI ISOLATES THIS IS TO OUR KNOWLEDGE THE FIRST SUCH GENOMIC COMPARISON H PYLORI WAS BELIEVED TO EXHIBIT A LARGE DEGREE OF GENOMIC AND ALLELIC DIVERSITY BUT WE FIND THAT THE OVERALL GENOMIC ORGANIZATION GENE ORDER AND PREDICTED PROTEOMES SETS OF PROTEINS ENCODED BY THE GENOMES OF THE TWO STRAINS ARE QUITE SIMILAR BETWEEN TO OF THE GENES ARE SPECIFIC TO EACH STRAIN WITH ALMOST HALF OF THESE GENES BEING CLUSTERED IN A SINGLE HYPERVARIABLE REGION']\n",
      " ['402' 'E'\n",
      "  'ANTIBODIES CAN BE USED TO IDENTIFY TISSUE- AND STAGE-SPECIFICALLY EXPRESSED GENES A MONOCLONAL ANTIBODY MAB AB49 FROM A HYBRIDOMA LIBRARY SCREENED FOR IMMUNOHISTOCHEMICAL STAINING IN THE ADULT NERVOUS SYSTEM OF DROSOPHILA MELANOGASTER WAS FOUND TO SELECTIVELY BIND TO ALL NEUROPIL REGIONS AND TO SYNAPTIC BOUTONS OF MOTOR NEURONS IN WESTERN BLOTS OF HOMOGENIZED BRAINS THE ANTIBODY RECOGNIZES TWO PROTEINS OF AND KD USING THIS ANTIBODY WE HAVE ISOLATED SEVEN CDNA CLONES THAT DERIVE FROM TWO POLYADENYLATED MRNA SPLICE VARIANTS OF A GENE LOCATED AT 79E1-2 ON POLYTENE CHROMOSOMES THE TWO MRNAS CODE FOR TWO INFERRED PROTEINS OF AND AMINO ACIDS RESPECTIVELY WHICH ARE IDENTICAL EXCEPT FOR THEIR C-TERMINALS AND A CENTRAL DELETION OF AMINO ACIDS IN THE SECOND PROTEIN BOTH CONTAIN A CONTIGUOUS STRING OF CYSTEINE RESIDUES IN SITU HYBRIDIZATION TO FROZEN HEAD SECTIONS DETECTS EXPRESSION OF THIS GENE IN RETINA AND NEURONAL PERIKARYA THE AND KD BRAIN PROTEINS THAT PRESUMABLY ARE LOCALIZED PREDOMINANTLY IN SYNAPTIC TERMINALS OF PHOTORECEPTORS AND MOST IF NOT ALL NEURONS MAY CORRESPOND TO TWO VARIANT CYSTEINE-STRING PROTEINS AS THEY ARE OF SIMILAR MOLECULAR WEIGHT AND SHARE AN ANTIGENIC BINDING SITE FOR MAB AB49']\n",
      " ['403' 'E'\n",
      "  'AMINO ACID SEQUENCES OF FIBRINOPEPTIDES A AND B FROM THE MACAQUE MACACA FUSCATA JAPANESE MONKEY AND THE GUENON ERYTHROCEBUS PATAS PATAS MONKEY WERE ESTABLISHED FIBRINOPEPTIDES A OF THE MONKEYS HAD A SEQUENCE IDENTICAL WITH THOSE OF BABOONS ALA-ASP-THR-GLY-GLU-GLY-ASP-PHE-LEU-ALA-GLU-GLY-GLY-GLY-VAL-ARG FIBRINOPEPTIDES B WERE 9-RESIDUE SHORT PEPTIDES WITH THE SEQUENCES ASN-GLU-GLU-SER-LEU-PHE-SER-GLY-ARG FOR M FUSCATA AND ASN-GLU-GLU-VAL-LEU-PHE-GLY-GLY-ARG FOR E PATAS THE SEQUENCE OF THE B PEPTIDE OF M FUSCATA DIFFERED FROM THAT OF A CLOSE-RELATED SPECIES M MULATTA RHESUS MONKEY AT A SINGLE SITE LEU MF----PRO MM A SINGLE REPLACEMENT BETWEEN THE B PEPTIDES OF E PATAS AND CERCOCEBUS AETHIOPS GREEN MONKEY VAL EP----GLY CA WAS DETECTED A PHYLOGENIC RELATIONSHIP OF MACAQUES GUENONS AND BABOONS NAMED CERCOPITHECINAE OLD WORLD MONKEY WAS DEDUCED FROM THE SEQUENCE DATA A SELECTIVE RATHER THAN RANDOM AMINO ACID REPLACEMENT WAS OBSERVED IN THE B PEPTIDES OF THESE OLD WORLD MONKEYS SUGGESTING A RESTRICTED MUTATION OF THEIR FIBRINOPEPTIDES DURING PRIMATE EVOLUTION']\n",
      " ...\n",
      " ['798' 'B'\n",
      "  'A GENE WHICH IS UNIQUE TO THE QPRS PLASMID FROM CHRONIC ISOLATES OF COXIELLA BURNETII WAS CLONED SEQUENCED AND EXPRESSED IN ESCHERICHIA COLI THIS GENE TERMED CBBE  CODES FOR A PUTATIVE SURFACE PROTEIN OF APPROXIMATELY KDA TERMED THE E  PROTEIN THE CBBE  GENE IS BP IN LENGTH AND IS PRECEDED BY PREDICTED PROMOTER REGULATORY SEQUENCES OF TTTAAT TATAAT AND A SHINE-DALGARNO SEQUENCE OF GGAGAGA ALL OF WHICH CLOSELY RESEMBLE THOSE OF E COLI AND OTHER RICKETTSIAE THE OPEN READING FRAME ORF OF CBBE  ENDS WITH A UAA CODON FOLLOWED BY A SECOND IN-FRAME UAG STOP CODON AND A REGION OF DYAD SYMMETRY WHICH MAY ACT AS A RHO-FACTOR-INDEPENDENT TERMINATOR THE ORF OF CBBE  IS CAPABLE OF CODING FOR A POLYPEPTIDE OF AMINO ACIDS WITH A PREDICTED MOLECULAR MASS OF DA THE E  PROTEIN HAS A PREDICTED PI OF APPROXIMATELY AND CONTAINS A DISTINCT HYDROPHOBIC REGION OF AMINO ACID RESIDUES IN VITRO TRANSCRIPTIONTRANSLATION AND E COLI EXPRESSION OF RECOMBINANT PLASMIDS CONTAINING CBBE  PRODUCE A PROTEIN OF APPROXIMATELY KDA THE IN VIVO EXPRESSION OF CBBE  YIELDS A NOVEL PROTEIN THAT CAN BE DETECTED ON IMMUNOBLOTS DEVELOPED WITH RABBIT ANTISERUM GENERATED AGAINST PURIFIED OUTER MEMBRANE FROM C BURNETII DNA HYBRIDIZATION ANALYSIS SHOWS THAT CBBE  IS UNIQUE TO THE QPRS PLASMID FOUND IN CHRONIC ISOLATES OF C BURNETII AND IS ABSENT IN CHROMOSOMAL DNA AND PLASMIDS QPH1 QPDG FROM OTHER ISOLATES OF C BURNETII A SEARCH OF VARIOUS DNA AND AMINO ACID SEQUENCE DATA BASES REVEALED NO HOMOLOGIES TO CBBE ']\n",
      " ['799' 'B'\n",
      "  'AN APPROACH FOR GENOME ANALYSIS BASED ON SEQUENCING AND ASSEMBLY OF UNSELECTED PIECES OF DNA FROM THE WHOLE CHROMOSOME HAS BEEN APPLIED TO OBTAIN THE COMPLETE NUCLEOTIDE SEQUENCE BASE PAIRS OF THE GENOME FROM THE BACTERIUM HAEMOPHILUS INFLUENZAE RD THIS APPROACH ELIMINATES THE NEED FOR INITIAL MAPPING EFFORTS AND IS THEREFORE APPLICABLE TO THE VAST ARRAY OF MICROBIAL SPECIES FOR WHICH GENOME MAPS ARE UNAVAILABLE THE H INFLUENZAE RD GENOME SEQUENCE GENOME SEQUENCE DATABASE ACCESSION NUMBER L42023 REPRESENTS THE ONLY COMPLETE GENOME SEQUENCE FROM A FREE-LIVING ORGANISM']\n",
      " ['800' 'E'\n",
      "  'A NOVEL STYLAR-SPECIFIC GLYCOSYLATED PROTEIN SP41 WAS CHARACTERIZED SP41 CONSTITUTES GREATER THAN OF THE TRANSMITTING TRACT TISSUE SOLUBLE PROTEINS AND IS MAINLY LOCALIZED IN THE EXTRACELLULAR MATRIX TWO CDNA CLONES CORRESPONDING TO SP41 MRNA WERE ISOLATED AND SEQUENCED THE DECODED SEQUENCES ARE RESPECTIVELY AND HOMOLOGOUS TO ACIDIC AND BASIC PATHOGEN-INDUCED 1-3-BETA-GLUCANASES OF THE LEAF THUS A SUBFAMILY OF 1-3-BETA-GLUCANASE PATHOGENESIS-RELATED PR PROTEINS CONSTITUTES ONE OF THE MAJOR STYLAR MATRIX PROTEINS THE ACCUMULATION OF SP41 TRANSCRIPTS IN NORMALLY DEVELOPING AND ELICITOR-TREATED STYLES AND LEAVES WAS FOLLOWED USING AN RNASE PROTECTION ASSAY DURING DEVELOPMENT SP41 TRANSCRIPT ACCUMULATION STARTS WELL AFTER CARPEL DIFFERENTIATION IT IS FIRST DETECTED IN STYLES AT DAYS BEFORE ANTHESIS THE MAXIMAL LEVEL OF ACCUMULATION IS REACHED DURING ANTHESIS ELICITOR-TREATED STYLES DO NOT ACCUMULATE THE LEAF-TYPE 1-3-BETA-GLUCANASE TRANSCRIPT ALTHOUGH THEY RETAIN THE CAPACITY TO SYNTHESIZE LEAF-TYPE PATHOGENESIS-RELATED PROTEINS SUCH AS THE PATHOGEN-INDUCED ACIDIC CHITINASE THE DEVELOPMENTAL REGULATION OF SP41 EXPRESSION POINTS TO A ROLE FOR THEM IN THE NORMAL PROCESSES OF FLOWERING AND REPRODUCTIVE PHYSIOLOGY']]\n"
     ]
    }
   ],
   "source": [
    "#Cross-Validation Test\n",
    "folds = 10\n",
    "samplesData = {}\n",
    "cycle = 0\n",
    "performance = np.zeros(folds)\n",
    "\n",
    "#Split data \n",
    "#Creating Folds\n",
    "foldSize= int(len(data)/folds)\n",
    "initialSample = 0\n",
    "finalSample = foldSize\n",
    "for fold in range(folds):\n",
    "    if fold == folds:\n",
    "        samplesData[fold] = data[initialSample:,0:3]\n",
    "    else:\n",
    "        samplesData[fold] = data[initialSample:finalSample,0:3]\n",
    "        initialSample = finalSample\n",
    "        finalSample += foldSize\n",
    "\n",
    "print(samplesData[1])\n",
    "#Evaluate model performances with cross-validation\n",
    "#for(cycle; cycle<folds; cycle++)\n",
    "#training = {}\n",
    "#for sample in samples:\n",
    "#    print(sample)\n",
    "#    test = samples[sample]\n",
    "#    for trainingSample in samples:\n",
    "#        print(trainingSample)\n",
    "#        if sample!=trainingSample:\n",
    "#            print(samples[sample])\n",
    "#            training = training.update(samples[sample])\n",
    "#    print(len(training))\n",
    "#    testFold = DataTraining[cycle]\n",
    "#    trainingFolds = DataTraining.pop(cycle)\n",
    "#    performance[cycle] = runtest(trainingFolds, testFold)\n",
    "#    cycle++ \n",
    "#cycle = 0\n",
    "#averagePerformance = np.sum(performance)/folds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "Fold lenght:3600\n",
      "Total number of words:27651\n",
      "Total number of words after remove unique :12358\n",
      "[0.395 0.    0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      "0.0395\n",
      "Fold: 1\n",
      "Fold lenght:3600\n",
      "Total number of words:27683\n",
      "Total number of words after remove unique :12279\n",
      "[0.395 0.41  0.    0.    0.    0.    0.    0.    0.    0.   ]\n",
      "0.08049999999999999\n",
      "Fold: 2\n",
      "Fold lenght:3600\n",
      "Total number of words:27590\n",
      "Total number of words after remove unique :12183\n",
      "[0.395 0.41  0.375 0.    0.    0.    0.    0.    0.    0.   ]\n",
      "0.118\n",
      "Fold: 3\n",
      "Fold lenght:3600\n",
      "Total number of words:27732\n",
      "Total number of words after remove unique :12299\n",
      "[0.395 0.41  0.375 0.41  0.    0.    0.    0.    0.    0.   ]\n",
      "0.15899999999999997\n",
      "Fold: 4\n",
      "Fold lenght:3600\n",
      "Total number of words:27770\n",
      "Total number of words after remove unique :12240\n",
      "[0.395 0.41  0.375 0.41  0.43  0.    0.    0.    0.    0.   ]\n",
      "0.202\n",
      "Fold: 5\n",
      "Fold lenght:3600\n",
      "Total number of words:27932\n",
      "Total number of words after remove unique :12321\n",
      "[0.395  0.41   0.375  0.41   0.43   0.3875 0.     0.     0.     0.    ]\n",
      "0.24075000000000002\n",
      "Fold: 6\n",
      "Fold lenght:3600\n",
      "Total number of words:27959\n",
      "Total number of words after remove unique :12319\n",
      "[0.395  0.41   0.375  0.41   0.43   0.3875 0.4225 0.     0.     0.    ]\n",
      "0.28300000000000003\n",
      "Fold: 7\n",
      "Fold lenght:3600\n",
      "Total number of words:27889\n",
      "Total number of words after remove unique :12271\n",
      "[0.395  0.41   0.375  0.41   0.43   0.3875 0.4225 0.41   0.     0.    ]\n",
      "0.324\n",
      "Fold: 8\n",
      "Fold lenght:3600\n",
      "Total number of words:27830\n",
      "Total number of words after remove unique :12334\n",
      "[0.395  0.41   0.375  0.41   0.43   0.3875 0.4225 0.41   0.36   0.    ]\n",
      "0.36\n",
      "Fold: 9\n",
      "Fold lenght:3600\n",
      "Total number of words:27803\n",
      "Total number of words after remove unique :12236\n",
      "[0.395  0.41   0.375  0.41   0.43   0.3875 0.4225 0.41   0.36   0.405 ]\n",
      "0.40049999999999997\n"
     ]
    }
   ],
   "source": [
    "#Cross-Validation Test\n",
    "folds = 10\n",
    "samplesData = {}\n",
    "cycle = 0\n",
    "performance = np.zeros(folds)\n",
    "\n",
    "#Split data \n",
    "#Creating Folds\n",
    "foldSize= int(len(data)/folds)\n",
    "initialSample = 0\n",
    "finalSample = foldSize\n",
    "for fold in range(folds):\n",
    "    if fold == folds:\n",
    "        samplesData[fold] = data[initialSample:,0:3]\n",
    "    else:\n",
    "        samplesData[fold] = data[initialSample:finalSample,0:3]\n",
    "        initialSample = finalSample\n",
    "        finalSample += foldSize\n",
    "        \n",
    "#Evaluate model performances with cross-validation\n",
    "for sample in range(folds):\n",
    "    trainingData = []   # id, class\n",
    "    print(\"Fold: \" + str(sample))\n",
    "    testData = samplesData[sample]     #id, class, abstracts\n",
    "    for trainingSample in samplesData:        \n",
    "        if sample!=trainingSample:\n",
    "            for x in range(len(samplesData[trainingSample])):\n",
    "                trainingData.append(samplesData[trainingSample][x])\n",
    "\n",
    "    trainingData = np.array(trainingData)\n",
    "    \n",
    "    #Determine Instances\n",
    "    numInstances = len(trainingData)\n",
    "    print(\"Fold lenght:\" + str(numInstances))\n",
    "    \n",
    "    #Preprocessing training Data\n",
    "    training = preprocessing(trainingData)\n",
    "    \n",
    "    #Select training\n",
    "    #training = abstracts\n",
    "    \n",
    "    #Determine classes\n",
    "    classes = np.unique(trainingData[:,1])\n",
    "    \n",
    "    #Determine class probabilities\n",
    "    classProbs = classProbability(trainingData[:,1])\n",
    "    \n",
    "    #Calculate words frequency\n",
    "    allWords = wordCollector(training)\n",
    "    print(\"Total number of words:\" + str(len(allWords)))\n",
    "    \n",
    "    allWords = removeUnique(allWords, 1)\n",
    "    print(\"Total number of words after remove unique :\" + str(len(allWords)))\n",
    "    \n",
    "    #Create and array with all words\n",
    "    words = list(allWords)\n",
    "    words.sort()\n",
    "    \n",
    "    #Get words count of instances of each class value\n",
    "    classesWords = {}\n",
    "    for c in classes:\n",
    "        classesWords[c] = classWords(c, training, np.where(trainingData==c)[0])\n",
    "    \n",
    "    predictions =[]\n",
    "    for elem in range(len(testData)):\n",
    "        \n",
    "        #Get conditional probabilities of rows\n",
    "        condProbs = conditionalProb(words,testData[elem],classes,classesWords)\n",
    "        \n",
    "        #Classifier - Selecting a class\n",
    "        classifier = classProb(testData[elem], condProbs, classProbs)\n",
    "        #Select final classification\n",
    "        predictions.append(max(classifier.keys(), key=(lambda k: classifier[k])))\n",
    "    \n",
    "    #Measure fold performance\n",
    "    performance[sample]= modelPerformance(predictions,testData)\n",
    "    averagePerformance = sum(performance)/len(performance)\n",
    "    print(performance)\n",
    "    print(averagePerformance)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelPerformance(predictions, real):\n",
    "    valid=0;\n",
    "    total = len(predictions)\n",
    "    for prediction in range(len(predictions)):\n",
    "        if predictions[prediction]==real[prediction][1]:\n",
    "            valid += 1\n",
    "    performance=valid/total\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Fold: 0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only size-1 arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-6d3ce2319b70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m                 \u001b[0mindex2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamplesData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainingSample\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m                 \u001b[0mtrainingData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamplesData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msamplesData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 \u001b[0mtrainingData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingData\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msamplesData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrainingSample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_axis_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_ndim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0msl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_nx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "#Evaluate model performances with cross-validation\n",
    "print(samplesData.keys())\n",
    "for sample in range(folds):\n",
    "    trainingData = []   # id, class\n",
    "    trainingData = 0\n",
    "    print(\"Fold: \" + str(sample))\n",
    "    testData = samplesData.pop(sample)     #id, class, abstracts\n",
    "    for trainingSample in samplesData.keys():       \n",
    "        print(trainingSample)\n",
    "        if trainingSample<len(samples)-1 and trainingSample > 0:\n",
    "            if trainingData==0:\n",
    "                index1 = list(samplesData.keys())[trainingSample]\n",
    "                index2 = list(samplesData.keys())[trainingSample-1]\n",
    "                print(index1)\n",
    "                trainingData = np.stack(samplesData[index1],samplesData[index2])\n",
    "            else:\n",
    "                trainingData = np.stack(trainingData,samplesData[trainingSample])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'to_cvs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-3f71f9247439>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Generate Solution and Export to CSV\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mRawData\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_cvs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'amon897.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5065\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5066\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5067\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5069\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'to_cvs'"
     ]
    }
   ],
   "source": [
    "#Generate Solution and Export to CSV\n",
    "RawData.to_cvs('amon897.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
